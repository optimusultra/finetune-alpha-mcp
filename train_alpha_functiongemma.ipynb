{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fine-tuning FunctionGemma for Alpha-MCP\n",
                "\n",
                "This notebook trains **Google FunctionGemma 270M** on the **Alpha-MCP** toolset.\n",
                "It is **checkpoint-aware** â€” you can stop and resume training at any point,\n",
                "and re-run with expanded datasets when `alpha-mcp` adds new tools.\n",
                "\n",
                "## Architecture\n",
                "- **Base Model**: `google/functiongemma-270m-it` (gated â€” must login)\n",
                "- **Method**: LoRA + SFT via `trl`\n",
                "- **Dataset**: `data/synthetic_training.jsonl` (1200+ synthetic tool-call examples)\n",
                "- **Format**: FunctionGemma `<start_function_call>call:name{...}<end_function_call>`\n",
                "- **Hardware**: GTX 1060 6GB compatible (fp16, batch=1)\n",
                "\n",
                "## Checkpoint Resume\n",
                "Set `RESUME_FROM_CHECKPOINT = True` and point `CHECKPOINT_DIR` at your latest checkpoint\n",
                "folder (e.g. `checkpoints/checkpoint-500`) to continue a previous run."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import json\n",
                "import os\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "from peft import LoraConfig, get_peft_model\n",
                "from trl import SFTTrainer, SFTConfig\n",
                "from datasets import load_dataset\n",
                "from huggingface_hub import login\n",
                "import time\n",
                "\n",
                "print(f'PyTorch: {torch.__version__}')\n",
                "print(f'CUDA: {torch.cuda.is_available()} | Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"}')\n",
                "print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB' if torch.cuda.is_available() else '')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. Config â€” Edit Here"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "MODEL_NAME   = 'google/functiongemma-270m-it'  # FunctionGemma 270M (gated)\n",
                "# MODEL_NAME = 'google/gemma-2-2b-it'           # Uncomment for Gemma 2 2B (better reasoning)\n",
                "\n",
                "# â”€â”€ Dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "DATASET_PATH  = 'data/synthetic_training.jsonl'\n",
                "\n",
                "# â”€â”€ Output & Checkpoints â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "OUTPUT_DIR    = 'outputs/alpha_functiongemma'\n",
                "CHECKPOINT_DIR = 'checkpoints'  # Checkpoints saved here every N steps\n",
                "\n",
                "# â”€â”€ Resume Training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "RESUME_FROM_CHECKPOINT = False  # Set True to resume from latest checkpoint\n",
                "\n",
                "# â”€â”€ LoRA Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "LORA_R         = 16\n",
                "LORA_ALPHA     = 16\n",
                "LORA_DROPOUT   = 0.05\n",
                "\n",
                "# â”€â”€ Training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "MAX_SEQ_LENGTH = 512\n",
                "BATCH_SIZE     = 1\n",
                "GRAD_ACCUM     = 8\n",
                "LEARNING_RATE  = 2e-4\n",
                "NUM_EPOCHS     = 3         # More epochs = better tool discrimination\n",
                "SAVE_STEPS     = 100       # Checkpoint every 100 steps\n",
                "WARMUP_STEPS   = 30\n",
                "\n",
                "print('âœ… Config loaded.')\n",
                "print(f'   Model:    {MODEL_NAME}')\n",
                "print(f'   Dataset:  {DATASET_PATH}')\n",
                "print(f'   Resume:   {RESUME_FROM_CHECKPOINT}')\n",
                "print(f'   Epochs:   {NUM_EPOCHS} | Save every: {SAVE_STEPS} steps')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. HuggingFace Login (Required for Gated Model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Accept the license at: https://huggingface.co/google/functiongemma-270m-it\n",
                "# Then get your token at: https://huggingface.co/settings/tokens\n",
                "login()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Generate Synthetic Training Data\n",
                "\n",
                "This runs the data generator. **Re-run this cell whenever alpha-mcp adds new tools.**\n",
                "The generator will produce a fresh dataset reflecting the new schemas."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import subprocess\n",
                "result = subprocess.run(\n",
                "    ['python', 'scripts/generate_data.py', '--n', '1200'],\n",
                "    capture_output=True, text=True\n",
                ")\n",
                "print(result.stdout)\n",
                "if result.returncode != 0:\n",
                "    print('âŒ Error:', result.stderr)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load Model & Tokenizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f'â³ Loading {MODEL_NAME}...')\n",
                "t0 = time.time()\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    torch_dtype=torch.float16,\n",
                "    device_map='auto',\n",
                "    trust_remote_code=True,\n",
                ")\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "print(f'âœ… Loaded in {time.time()-t0:.1f}s')\n",
                "print(f'   VRAM after load: {torch.cuda.memory_allocated()/1024**3:.2f} GB')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Pre-Training Baseline (Tool Call Quality)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "TEST_PROMPTS = [\n",
                "    'What are the best Diamond setups in the S&P 500 right now?',\n",
                "    'Give me a full technical breakdown on NVDA.',\n",
                "    'Is the market in bull or bear mode?',\n",
                "    'Run a risk audit on my positions: AAPL, TSLA, MSFT.',\n",
                "    'Diamond screen the NASDAQ 100.',\n",
                "]\n",
                "\n",
                "print('=== PRE-TRAINING BASELINE ===')\n",
                "model.eval()\n",
                "for prompt in TEST_PROMPTS:\n",
                "    inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n",
                "    with torch.no_grad():\n",
                "        out = model.generate(**inputs, max_new_tokens=80, pad_token_id=tokenizer.eos_token_id)\n",
                "    resp = tokenizer.decode(out[0], skip_special_tokens=False)\n",
                "    print(f'\\nğŸ”¹ {prompt}')\n",
                "    print(f'ğŸ”¸ {resp.strip()}')\n",
                "    print('-'*50)\n",
                "model.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Apply LoRA Adapters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lora_config = LoraConfig(\n",
                "    r=LORA_R,\n",
                "    lora_alpha=LORA_ALPHA,\n",
                "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj',\n",
                "                    'gate_proj', 'up_proj', 'down_proj'],\n",
                "    lora_dropout=LORA_DROPOUT,\n",
                "    bias='none',\n",
                "    task_type='CAUSAL_LM',\n",
                ")\n",
                "model = get_peft_model(model, lora_config)\n",
                "model.print_trainable_parameters()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Prepare Dataset\n",
                "\n",
                "Formats each conversation into the FunctionGemma `<start_of_turn>` template."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "raw_dataset = load_dataset('json', data_files=DATASET_PATH, split='train')\n",
                "print(f'âœ… Loaded {len(raw_dataset)} training examples')\n",
                "\n",
                "def format_conversation(example):\n",
                "    \"\"\"Convert ShareGPT format â†’ FunctionGemma turn format.\"\"\"\n",
                "    text = ''\n",
                "    for turn in example['conversations']:\n",
                "        role = turn['role']\n",
                "        content = turn['content']\n",
                "        text += f'<start_of_turn>{role}\\n{content}<end_of_turn>\\n'\n",
                "    text += tokenizer.eos_token\n",
                "    return {'text': text}\n",
                "\n",
                "dataset = raw_dataset.map(format_conversation, batched=False)\n",
                "\n",
                "# Quick sample inspection\n",
                "print('\\n--- Sample 0 ---')\n",
                "print(dataset[0]['text'])\n",
                "print('--- Sample 1 ---')\n",
                "print(dataset[1]['text'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Train (Checkpoint-Aware)\n",
                "\n",
                "Training saves checkpoints every `SAVE_STEPS` steps.\n",
                "Set `RESUME_FROM_CHECKPOINT = True` to resume from the latest."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Detect latest checkpoint for resume\n",
                "resume_checkpoint = None\n",
                "if RESUME_FROM_CHECKPOINT:\n",
                "    checkpoints = sorted([\n",
                "        os.path.join(CHECKPOINT_DIR, d) for d in os.listdir(CHECKPOINT_DIR)\n",
                "        if d.startswith('checkpoint-')\n",
                "    ], key=lambda x: int(x.split('-')[-1]))\n",
                "    if checkpoints:\n",
                "        resume_checkpoint = checkpoints[-1]\n",
                "        print(f'â–¶ï¸  Resuming from: {resume_checkpoint}')\n",
                "    else:\n",
                "        print('âš ï¸  No checkpoints found, starting fresh.')\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    processing_class=tokenizer,\n",
                "    train_dataset=dataset,\n",
                "    args=SFTConfig(\n",
                "        dataset_text_field='text',\n",
                "        max_length=MAX_SEQ_LENGTH,\n",
                "        per_device_train_batch_size=BATCH_SIZE,\n",
                "        gradient_accumulation_steps=GRAD_ACCUM,\n",
                "        warmup_steps=WARMUP_STEPS,\n",
                "        num_train_epochs=NUM_EPOCHS,\n",
                "        learning_rate=LEARNING_RATE,\n",
                "        fp16=True,\n",
                "        bf16=False,\n",
                "        logging_steps=10,\n",
                "        optim='adamw_torch',\n",
                "        weight_decay=0.01,\n",
                "        lr_scheduler_type='cosine',\n",
                "        seed=3407,\n",
                "        output_dir=CHECKPOINT_DIR,  # Checkpoints go here\n",
                "        save_strategy='steps',\n",
                "        save_steps=SAVE_STEPS,\n",
                "        save_total_limit=5,          # Keep last 5 checkpoints\n",
                "        load_best_model_at_end=False,\n",
                "        report_to='none',\n",
                "    ),\n",
                ")\n",
                "\n",
                "print(f'ğŸš€ Starting training ({NUM_EPOCHS} epochs, {len(dataset)} samples, resume={RESUME_FROM_CHECKPOINT})')\n",
                "t0 = time.time()\n",
                "trainer.train(resume_from_checkpoint=resume_checkpoint)\n",
                "print(f'âœ… Training complete in {(time.time()-t0)/60:.1f} min')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Post-Training Verification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('=== POST-TRAINING TOOL CALL QUALITY ===')\n",
                "model.eval()\n",
                "for prompt in TEST_PROMPTS:\n",
                "    inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n",
                "    with torch.no_grad():\n",
                "        out = model.generate(**inputs, max_new_tokens=80, pad_token_id=tokenizer.eos_token_id)\n",
                "    resp = tokenizer.decode(out[0], skip_special_tokens=False)\n",
                "    has_call = '<start_function_call>' in resp\n",
                "    status = 'âœ…' if has_call else 'âŒ'\n",
                "    print(f'{status} {prompt}')\n",
                "    if has_call:\n",
                "        call_part = resp.split('<start_function_call>')[1].split('<end_function_call>')[0]\n",
                "        print(f'   â†’ {call_part}')\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Save Final Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "model.save_pretrained(OUTPUT_DIR)\n",
                "tokenizer.save_pretrained(OUTPUT_DIR)\n",
                "print(f'ğŸ Final adapter saved to: {OUTPUT_DIR}')\n",
                "\n",
                "# Save training metadata for reproducibility\n",
                "import json\n",
                "meta = {\n",
                "    'base_model': MODEL_NAME,\n",
                "    'dataset': DATASET_PATH,\n",
                "    'num_samples': len(dataset),\n",
                "    'epochs': NUM_EPOCHS,\n",
                "    'lora_r': LORA_R,\n",
                "    'lora_alpha': LORA_ALPHA,\n",
                "    'timestamp': time.strftime('%Y-%m-%dT%H:%M:%S'),\n",
                "    'tools': ['analyze_ticker_full', 'run_diamond_screen', 'get_market_pulse', 'run_risk_audit'],\n",
                "}\n",
                "with open(f'{OUTPUT_DIR}/training_metadata.json', 'w') as f:\n",
                "    json.dump(meta, f, indent=2)\n",
                "print(f'Metadata: {OUTPUT_DIR}/training_metadata.json')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "unsloth_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}