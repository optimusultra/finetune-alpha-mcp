# ğŸ¦ `finetune_alpha_mcp` â€” Alpha-MCP Ã— FunctionGemma Fine-tuning

> Fine-tune **Google FunctionGemma 270M** (or Gemma 2 2B) to natively trigger `alpha-mcp` tools â€” turning a plain language request like *"How is NVDA looking?"* into a perfectly structured tool call with zero hallucination.

---

## ğŸ§  What This Does

The `alpha-mcp` server exposes 4 precision tools for institutional-grade market analysis. Out-of-the-box, general language models don't know these tools exist. This package fine-tunes a tiny, fast model to **reliably and unambiguously dispatch the correct tool** given a natural language prompt â€” with proper arguments, correct schema, and no confusion between similar intents.

**Target accuracy: â‰¥ 85% on the 15-prompt eval suite.**

---

## ğŸ“¦ Package Structure

```
finetune_alpha_mcp/
â”œâ”€â”€ train.py                           â† Standalone CLI training script
â”œâ”€â”€ DEPLOYMENT.md                      â† Basic deployment options
â”œâ”€â”€ INTEGRATION_PLAN.md                â† Detailed Skill + Micro-script architecture
â”œâ”€â”€ tool_schemas.py                    â† Single source of truth for ALL tool definitions
â”œâ”€â”€ pyproject.toml                     â† uv project config (dependency management)
â”œâ”€â”€ setup_env.sh                       â† One-shot bootstrap for the isolated environment
â”œâ”€â”€ requirements.txt                   â† Alternative pip requirements
â”‚
â”œâ”€â”€ train_alpha_functiongemma.ipynb    â† Interactive training notebook (same as train.py)
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_data.py               â† Synthetic training data generator (1200 examples)
â”‚   â””â”€â”€ verify_model.py                â† Post-training accuracy scorecard
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ synthetic_training.jsonl       â† Pre-generated training set (auto-created)
â”‚   â””â”€â”€ tool_api_contract.json         â† Human-readable summary of all tool signatures
â”‚
â”œâ”€â”€ checkpoints/                       â† Auto-saved training checkpoints (gitignored)
â””â”€â”€ outputs/
    â””â”€â”€ alpha_functiongemma/           â† Final LoRA adapter (gitignored)
```

---

## ğŸš€ Quickstart

### 1. Set Up the Isolated Environment (uv)

```bash
cd /home/mihir/projects/finetune_alpha_mcp

# Create .venv + install CUDA-enabled PyTorch + ML stack:
bash setup_env.sh
```

> This creates a `.venv` directory isolated from your system Python and installs all dependencies (torch, transformers, trl, peft, datasets, accelerate) pinned to compatible versions.

### 2. Accept the HuggingFace License

`FunctionGemma 270M` is a **gated model**. You must:
1. Go to [huggingface.co/google/functiongemma-270m-it](https://huggingface.co/google/functiongemma-270m-it)
2. Click **"Access Repository"** and accept the license.
3. Get a token from [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)

```bash
export HF_TOKEN=hf_your_token_here
```

### 3. Generate Synthetic Training Data

```bash
uv run python3 scripts/generate_data.py --n 1200
```

> Or just run `train.py` with the `--regen-data` flag to do it automatically.

### 4. Train

```bash
# Default: 3 epochs, FunctionGemma 270M, checkpoint every 100 steps
uv run python3 train.py

# Resume from last checkpoint (after a crash or interruption):
uv run python3 train.py --resume

# More epochs for better accuracy:
uv run python3 train.py --epochs 5

# Re-generate fresh data + train in one shot:
uv run python3 train.py --regen-data --epochs 3
```

### 5. Verify

```bash
uv run python3 scripts/verify_model.py --model outputs/alpha_functiongemma
```

Expected output:
```
âœ… [analyze_ticker_full] â† 'Give me the full analysis on NVDA.'
âœ… [run_diamond_screen]  â† 'Run a diamond screen across the S&P 500.'
âœ… [get_market_pulse]    â† 'What's the current market regime?'
âœ… [run_risk_audit]      â† 'Risk audit my portfolio: AAPL, TSLA, MSFT.'
...
Tool Call Accuracy: 14/15 = 93.3%
ğŸ† PASS â€” Model is ready for deployment!
```

---

## ğŸ›  Training Configuration

Key parameters in `train.py` (all overridable via CLI):

| Parameter | Default | Description |
|:---|:---|:---|
| `--model` | `google/functiongemma-270m-it` | Base model |
| `--epochs` | `3` | Training epochs |
| `--lr` | `2e-4` | Learning rate |
| `--max-len` | `512` | Max token length per sample |
| `--save-steps` | `100` | Checkpoint frequency |
| `--resume` | `False` | Auto-resume from latest checkpoint |
| `--regen-data` | `False` | Regenerate synthetic data before training |

### LoRA Configuration (in `DEFAULTS`)

```python
lora_r       = 16   # Rank â€” increase to 32 for more capacity
lora_alpha   = 16   # Scaling
lora_dropout = 0.05 # Regularization
```

### VRAM Budget (GTX 1060 6GB)

| Stage | VRAM |
|:---|:---|
| Base model loaded | ~1.0 GB (270M fp16) |
| + LoRA adapters | ~1.2 GB |
| + Training batch | ~2.5â€“3.5 GB |
| Peak | â‰¤ 5 GB âœ… |

---

## ğŸ“Š Synthetic Dataset

The data generator (`scripts/generate_data.py`) produces **1,200 realistic training conversations** weighted by expected real-world usage:

| Tool | Samples | % |
|:---|:---:|:---:|
| `analyze_ticker_full` | ~492 | 41% |
| `run_diamond_screen` | ~338 | 28% |
| `get_market_pulse` | ~251 | 21% |
| `run_risk_audit` | ~119 | 10% |
| Disambiguation (multi-intent) | ~60 | 5% |

**Tickers used**: 40 real symbols across S&P 500 mega-caps, NASDAQ 100, mid-cap, and Russell small-caps â€” ensuring the model generalizes beyond just `AAPL` and `TSLA`.

**Format**: ShareGPT â†’ FunctionGemma `<start_of_turn>` / `<start_function_call>call:name{...}<end_function_call>` token sequence.

---

## â™»ï¸ Expanding When Alpha-MCP Grows

When `server.py` adds a new tool, the model needs to learn it. Here's the workflow to continue training without losing what the model already knows:

### Step 1 â€” Register the new tool in `tool_schemas.py`
```python
# In the EXPANSION ZONE at the bottom of ALPHA_MCP_TOOLS:
{
    "name": "get_earnings_calendar",
    "description": "Returns upcoming earnings dates and consensus estimates for a ticker.",
    "parameters": {
        "type": "object",
        "properties": {
            "ticker": {"type": "string", "description": "Stock symbol"},
            "weeks_ahead": {"type": "integer", "description": "How many weeks to look ahead"}
        },
        "required": ["ticker"]
    }
}
```

### Step 2 â€” Add prompt templates in `scripts/generate_data.py`
```python
def gen_get_earnings_calendar():
    t = random.choice(ALL_TICKERS)
    templates = [
        f"When does {t} report earnings?",
        f"Check the earnings calendar for {t}.",
        ...
    ]
    return build_sample(random.choice(templates), "get_earnings_calendar", {"ticker": t})
```

### Step 3 â€” Resume train on expanded dataset
```bash
uv run python3 train.py --regen-data --resume --epochs 2
```

> `--resume` ensures the model continues from the last checkpoint, not from scratch. The existing tool knowledge is preserved.

---

## ğŸ” Model Comparison

| Base Model | Size | VRAM | Speed | Tool Accuracy (est.) |
|:---|:---:|:---:|:---:|:---:|
| `functiongemma-270m-it` (**default**) | 270M | ~2.5 GB | Fast | ~85â€“92% |
| `gemma-2-2b-it` | 2B | ~4.5 GB | Slower | ~90â€“96% |

Switch to 2B for higher accuracy at the cost of slower inference:
```bash
uv run python3 train.py --model google/gemma-2-2b-it --epochs 3
```

---

## ğŸ”— Related

- **Alpha-MCP Server**: `/home/mihir/projects/openclaw/mcp-servers/alpha-mcp/`
- **FunctionGemma HF Page**: [google/functiongemma-270m-it](https://huggingface.co/google/functiongemma-270m-it)
- **FunctionGemma Docs**: [ai.google.dev](https://ai.google.dev/gemma/docs/function_gemma)

---

*Built for the Optimus Intelligence Workstation. ğŸ¦ğŸ›¡ï¸ğŸ“ˆ*
